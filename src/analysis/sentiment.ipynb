{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../data/yt_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m classifiers \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m   transformers\u001b[39m.\u001b[39mpipeline(\u001b[39m'\u001b[39m\u001b[39msentiment-analysis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m----> 3\u001b[0m   transformers\u001b[39m.\u001b[39;49mpipeline(\u001b[39m'\u001b[39;49m\u001b[39msentiment-analysis\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcardiffnlp/twitter-xlm-roberta-base-sentiment\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m      4\u001b[0m   transformers\u001b[39m.\u001b[39mpipeline(\u001b[39m'\u001b[39m\u001b[39msentiment-analysis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mj-hartmann/emotion-english-distilroberta-base\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      7\u001b[0m classify \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m inpt: [\u001b[39mcls\u001b[39m(inpt[:\u001b[39m400\u001b[39m]) \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m classifiers]\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/pipelines/__init__.py:873\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    870\u001b[0m             tokenizer_identifier \u001b[39m=\u001b[39m tokenizer\n\u001b[1;32m    871\u001b[0m             tokenizer_kwargs \u001b[39m=\u001b[39m model_kwargs\n\u001b[0;32m--> 873\u001b[0m         tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    874\u001b[0m             tokenizer_identifier, use_fast\u001b[39m=\u001b[39;49muse_fast, _from_pipeline\u001b[39m=\u001b[39;49mtask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokenizer_kwargs\n\u001b[1;32m    875\u001b[0m         )\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m load_image_processor:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# Try to infer image processor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mif\u001b[39;00m image_processor \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:697\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    695\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 697\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1804\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   1805\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   1806\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   1807\u001b[0m     init_configuration,\n\u001b[1;32m   1808\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   1809\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1810\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1811\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1812\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   1813\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1814\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1958\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1956\u001b[0m \u001b[39m# Instantiate tokenizer.\u001b[39;00m\n\u001b[1;32m   1957\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1958\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   1959\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   1960\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py:155\u001b[0m, in \u001b[0;36mXLMRobertaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m ):\n\u001b[1;32m    152\u001b[0m     \u001b[39m# Mask token behave like a normal word, i.e. include the space before it\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     mask_token \u001b[39m=\u001b[39m AddedToken(mask_token, lstrip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, rstrip\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mstr\u001b[39m) \u001b[39melse\u001b[39;00m mask_token\n\u001b[0;32m--> 155\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    156\u001b[0m         vocab_file,\n\u001b[1;32m    157\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    158\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    159\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    160\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    161\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    162\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    163\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    164\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    165\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    166\u001b[0m     )\n\u001b[1;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39m=\u001b[39m vocab_file\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_save_slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_file \u001b[39melse\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/y3_ads/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "  transformers.pipeline('sentiment-analysis', 'distilbert-base-uncased-finetuned-sst-2-english'),\n",
    "  transformers.pipeline('sentiment-analysis', 'cardiffnlp/twitter-xlm-roberta-base-sentiment'),\n",
    "  transformers.pipeline('sentiment-analysis', 'j-hartmann/emotion-english-distilroberta-base'),\n",
    "]\n",
    "\n",
    "classify = lambda inpt: [cls(inpt[:400]) for cls in classifiers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>vid_id</th>\n",
       "      <th>vid_title</th>\n",
       "      <th>vid_date</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>UgyMYmRpiaALxlXalop4AaABAg</td>\n",
       "      <td>Star Trek Emergence Season 7, episode 23 may b...</td>\n",
       "      <td>UCKqxhr_SUHw59C5ryXDyESw</td>\n",
       "      <td>2023-03-15T05:25:15Z</td>\n",
       "      <td>OhCzX0iLnOc</td>\n",
       "      <td>The danger of AI is weirder than you think | J...</td>\n",
       "      <td>2019-11-13T23:02:20Z</td>\n",
       "      <td>AU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Ugx5Nzd6Z3wNgEP9b394AaABAg</td>\n",
       "      <td>Her presentation skills are similar to the hum...</td>\n",
       "      <td>UCQphwD5T37LFXQgyfklvPMA</td>\n",
       "      <td>2023-02-08T06:33:25Z</td>\n",
       "      <td>OhCzX0iLnOc</td>\n",
       "      <td>The danger of AI is weirder than you think | J...</td>\n",
       "      <td>2019-11-13T23:02:20Z</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>UgyJ32d9gtRFpBclgGt4AaABAg</td>\n",
       "      <td>\"It doesn't know what a human is\"\\nWhich is am...</td>\n",
       "      <td>UCZOavABdxr61dMMxkRZXUdA</td>\n",
       "      <td>2023-01-25T03:03:47Z</td>\n",
       "      <td>OhCzX0iLnOc</td>\n",
       "      <td>The danger of AI is weirder than you think | J...</td>\n",
       "      <td>2019-11-13T23:02:20Z</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>UgyC7vlEJR2djI2REbF4AaABAg</td>\n",
       "      <td>\"Based on the data we have, hiring women is a ...</td>\n",
       "      <td>UCPZYOveDXQQBe16JnOH0ZRQ</td>\n",
       "      <td>2023-01-24T00:14:58Z</td>\n",
       "      <td>OhCzX0iLnOc</td>\n",
       "      <td>The danger of AI is weirder than you think | J...</td>\n",
       "      <td>2019-11-13T23:02:20Z</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>UgyXxc37Xf9sLlNZ5Sp4AaABAg</td>\n",
       "      <td>Janelle is simplifying a complex subject matte...</td>\n",
       "      <td>UCS9_kMzgw9WfLpF4--H3TmQ</td>\n",
       "      <td>2023-01-14T14:08:28Z</td>\n",
       "      <td>OhCzX0iLnOc</td>\n",
       "      <td>The danger of AI is weirder than you think | J...</td>\n",
       "      <td>2019-11-13T23:02:20Z</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>19576</td>\n",
       "      <td>Ugyd5BbYq-OdskoL32R4AaABAg</td>\n",
       "      <td>❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️</td>\n",
       "      <td>UCeI7PLAl7SkfzKshn09FybA</td>\n",
       "      <td>2018-08-22T19:03:17Z</td>\n",
       "      <td>0kxtPt_yi6w</td>\n",
       "      <td>Mitchie M - Burenai Ai De (ft. Hatsune Miku) [...</td>\n",
       "      <td>2018-08-22T19:00:01Z</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19579</th>\n",
       "      <td>19579</td>\n",
       "      <td>UgzBkjBd4EFCd2abH5N4AaABAg</td>\n",
       "      <td>Gặp ở Việt Nam thì nấu cháo đậu xanh hết chị k...</td>\n",
       "      <td>UCM4D5F1IYj42CPo0_WSus9Q</td>\n",
       "      <td>2023-02-14T15:43:03Z</td>\n",
       "      <td>KLqryygc8YU</td>\n",
       "      <td>Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...</td>\n",
       "      <td>2018-08-23T04:16:46Z</td>\n",
       "      <td>VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>19582</td>\n",
       "      <td>UgzripeLsJDirPbH9PV4AaABAg</td>\n",
       "      <td>Quay lại video đầu tiên của chị kính lúp 😙❤️</td>\n",
       "      <td>UCwHnLMwJvpJQSd8SZv-dC_Q</td>\n",
       "      <td>2022-12-27T10:44:51Z</td>\n",
       "      <td>KLqryygc8YU</td>\n",
       "      <td>Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...</td>\n",
       "      <td>2018-08-23T04:16:46Z</td>\n",
       "      <td>VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19589</th>\n",
       "      <td>19589</td>\n",
       "      <td>UgxJVsX2A7dlF8F4P9J4AaABAg</td>\n",
       "      <td>video hay quá ae🎉😂🎉</td>\n",
       "      <td>UC73u6NCjtWNr2subeWTzreA</td>\n",
       "      <td>2022-11-13T02:42:59Z</td>\n",
       "      <td>KLqryygc8YU</td>\n",
       "      <td>Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...</td>\n",
       "      <td>2018-08-23T04:16:46Z</td>\n",
       "      <td>VN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19631</th>\n",
       "      <td>19631</td>\n",
       "      <td>Ugy6W_qWDXtMy47DNAZ4AaABAg</td>\n",
       "      <td>Mở khoá nick fb hộ tao cái :))</td>\n",
       "      <td>UCyE_2Y-l6TQcbOhMDOMIiYg</td>\n",
       "      <td>2018-06-12T10:07:09Z</td>\n",
       "      <td>FUW6PV-InPQ</td>\n",
       "      <td>Mở Khóa ACC Free Fire Vĩnh Viễn ⏩ Có Ai Làm Đư...</td>\n",
       "      <td>2018-05-17T14:28:48Z</td>\n",
       "      <td>SG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2402 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                          id  \\\n",
       "3               3  UgyMYmRpiaALxlXalop4AaABAg   \n",
       "22             22  Ugx5Nzd6Z3wNgEP9b394AaABAg   \n",
       "30             30  UgyJ32d9gtRFpBclgGt4AaABAg   \n",
       "32             32  UgyC7vlEJR2djI2REbF4AaABAg   \n",
       "35             35  UgyXxc37Xf9sLlNZ5Sp4AaABAg   \n",
       "...           ...                         ...   \n",
       "19576       19576  Ugyd5BbYq-OdskoL32R4AaABAg   \n",
       "19579       19579  UgzBkjBd4EFCd2abH5N4AaABAg   \n",
       "19582       19582  UgzripeLsJDirPbH9PV4AaABAg   \n",
       "19589       19589  UgxJVsX2A7dlF8F4P9J4AaABAg   \n",
       "19631       19631  Ugy6W_qWDXtMy47DNAZ4AaABAg   \n",
       "\n",
       "                                                    text  \\\n",
       "3      Star Trek Emergence Season 7, episode 23 may b...   \n",
       "22     Her presentation skills are similar to the hum...   \n",
       "30     \"It doesn't know what a human is\"\\nWhich is am...   \n",
       "32     \"Based on the data we have, hiring women is a ...   \n",
       "35     Janelle is simplifying a complex subject matte...   \n",
       "...                                                  ...   \n",
       "19576                           ❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️❤️   \n",
       "19579  Gặp ở Việt Nam thì nấu cháo đậu xanh hết chị k...   \n",
       "19582       Quay lại video đầu tiên của chị kính lúp 😙❤️   \n",
       "19589                                video hay quá ae🎉😂🎉   \n",
       "19631                     Mở khoá nick fb hộ tao cái :))   \n",
       "\n",
       "                        channel                  date       vid_id  \\\n",
       "3      UCKqxhr_SUHw59C5ryXDyESw  2023-03-15T05:25:15Z  OhCzX0iLnOc   \n",
       "22     UCQphwD5T37LFXQgyfklvPMA  2023-02-08T06:33:25Z  OhCzX0iLnOc   \n",
       "30     UCZOavABdxr61dMMxkRZXUdA  2023-01-25T03:03:47Z  OhCzX0iLnOc   \n",
       "32     UCPZYOveDXQQBe16JnOH0ZRQ  2023-01-24T00:14:58Z  OhCzX0iLnOc   \n",
       "35     UCS9_kMzgw9WfLpF4--H3TmQ  2023-01-14T14:08:28Z  OhCzX0iLnOc   \n",
       "...                         ...                   ...          ...   \n",
       "19576  UCeI7PLAl7SkfzKshn09FybA  2018-08-22T19:03:17Z  0kxtPt_yi6w   \n",
       "19579  UCM4D5F1IYj42CPo0_WSus9Q  2023-02-14T15:43:03Z  KLqryygc8YU   \n",
       "19582  UCwHnLMwJvpJQSd8SZv-dC_Q  2022-12-27T10:44:51Z  KLqryygc8YU   \n",
       "19589  UC73u6NCjtWNr2subeWTzreA  2022-11-13T02:42:59Z  KLqryygc8YU   \n",
       "19631  UCyE_2Y-l6TQcbOhMDOMIiYg  2018-06-12T10:07:09Z  FUW6PV-InPQ   \n",
       "\n",
       "                                               vid_title  \\\n",
       "3      The danger of AI is weirder than you think | J...   \n",
       "22     The danger of AI is weirder than you think | J...   \n",
       "30     The danger of AI is weirder than you think | J...   \n",
       "32     The danger of AI is weirder than you think | J...   \n",
       "35     The danger of AI is weirder than you think | J...   \n",
       "...                                                  ...   \n",
       "19576  Mitchie M - Burenai Ai De (ft. Hatsune Miku) [...   \n",
       "19579  Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...   \n",
       "19582  Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...   \n",
       "19589  Có Cho Tiền Tỷ Cũng Không Ai Dám Đến ...   \n",
       "19631  Mở Khóa ACC Free Fire Vĩnh Viễn ⏩ Có Ai Làm Đư...   \n",
       "\n",
       "                   vid_date country  \n",
       "3      2019-11-13T23:02:20Z      AU  \n",
       "22     2019-11-13T23:02:20Z      CA  \n",
       "30     2019-11-13T23:02:20Z      US  \n",
       "32     2019-11-13T23:02:20Z      US  \n",
       "35     2019-11-13T23:02:20Z      US  \n",
       "...                     ...     ...  \n",
       "19576  2018-08-22T19:00:01Z      LB  \n",
       "19579  2018-08-23T04:16:46Z      VN  \n",
       "19582  2018-08-23T04:16:46Z      VN  \n",
       "19589  2018-08-23T04:16:46Z      VN  \n",
       "19631  2018-05-17T14:28:48Z      SG  \n",
       "\n",
       "[2402 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geodata = df[df['country'].notna()]\n",
    "geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'NEGATIVE', 'score': 0.9878159165382385},\n",
       "  {'label': 'POSITIVE', 'score': 0.9992153644561768},\n",
       "  {'label': 'POSITIVE', 'score': 0.9975312948226929},\n",
       "  {'label': 'NEGATIVE', 'score': 0.7892680764198303},\n",
       "  {'label': 'NEGATIVE', 'score': 0.6970564126968384},\n",
       "  {'label': 'NEGATIVE', 'score': 0.6635550856590271},\n",
       "  {'label': 'NEGATIVE', 'score': 0.9983240962028503},\n",
       "  {'label': 'NEGATIVE', 'score': 0.9925286769866943},\n",
       "  {'label': 'NEGATIVE', 'score': 0.9948663711547852},\n",
       "  {'label': 'NEGATIVE', 'score': 0.996830403804779}],\n",
       " [{'label': 'surprise', 'score': 0.8377178311347961},\n",
       "  {'label': 'neutral', 'score': 0.7586130499839783},\n",
       "  {'label': 'surprise', 'score': 0.48125723004341125},\n",
       "  {'label': 'neutral', 'score': 0.8989964723587036},\n",
       "  {'label': 'neutral', 'score': 0.6920442581176758},\n",
       "  {'label': 'neutral', 'score': 0.3846134543418884},\n",
       "  {'label': 'surprise', 'score': 0.6735503077507019},\n",
       "  {'label': 'neutral', 'score': 0.7688688039779663},\n",
       "  {'label': 'disgust', 'score': 0.7290061712265015},\n",
       "  {'label': 'neutral', 'score': 0.8750044107437134}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify(geodata['text'][20:30].to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y3_ads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
